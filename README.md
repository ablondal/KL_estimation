# KL Divergence Estimation

Efficient KL divergence estimation using importance sampling with various proposal distributions for language model distributions.

## Installation

```bash
pip install numpy scipy matplotlib torch transformers
```

---

## Files

- `Synthetic_tests.py` - Main synthetic experiments
- `visualize_proposals.py` - Visualization utilities (required)
<!-- - `LLM_experiments.py` - Real LLM experiments -->

---
## Synthetic Experiments

Tests importance sampling proposals on controlled long-tailed distributions that mimic realistic language model token distributions.

### Usage

```bash
python Synthetic_tests.py
```

### What it does:
1. Creates base (P) and fine-tuned (Q) distributions using three different models:
   - **Zipfian**: Simple power-law distribution
   - **Zipfian with Cutoff**: Power-law head + exponential tail
   - **Mixture**: Complex three-component distribution (most realistic)
2. Generates multiple proposal distributions for importance sampling
3. Computes true KL(P||Q) for comparison
4. Estimates KL using naive Monte Carlo and all implemented proposals
5. Ranks methods by accuracy, variance, and combined score
6. Generates comprehensive visualizations

### Configuration

Edit parameters in `__main__`:
```python
vocab_size = 3000              # Vocabulary size
n_samples = 300                # Samples per method
n_repeats = 2                  # Experimental repetitions
divergence = 'medium'          # 'low', 'medium', or 'high'
alpha_values = [0.3, 0.5, 0.7, 0.9]  # Parameters to test
```

### Distribution Types

**Zipfian (create_model_pair)**
- Simple: P(k) ∝ k^(-1.5)
- Models basic word frequency patterns

**Zipfian with Cutoff (create_model_pair_zipCutoff)**
- Power-law head + exponential tail decay
- Models finite context effects in LLMs

**Mixture (create_model_pair_mixture)**
- Heavy head (top 100 tokens) + power-law middle + flat tail
- Most realistic model of actual LLM distributions

### Divergence Levels
- **Low**: 10% probability mass shift between P and Q
- **Medium**: 30% probability mass shift
- **High**: Different α parameter + 50% shift

---

## Methods Implemented

### Baseline
- **Naive MC**: Samples from P, estimates E_P[log(P/Q)]
- **Rao-Blackwellized MC**: 

### Importance Sampling Proposals

1. **Alpha-mixture** (α ∈ {0.3, 0.5, 0.7, 0.9}):
   ```
   r = α·P|log(P/Q)| + (1-α)·P
   ```
   Explicitly targets high-divergence regions

2. **Geometric mixture** (β ∈ {0.3, 0.5, 0.7, 0.9}):
   ```
   r ∝ P^β·Q^(1-β)
   ```
   Smoothly interpolates between P and Q

3. **Fixed mixture** (λ ∈ {0.3, 0.5, 0.7, 0.9}):
   ```
   r = λP + (1-λ)Q
   ```
   Simple linear combination

4. **Chi-square aware** (α ∈ {0.3, 0.5, 0.7, 0.9}):
   ```
   r ∝ P·(P/Q)^α
   ```
   Optimal for χ² divergence when α=0.5

5. **Cross-entropy tilted** (temperature ∈ {0.3, 0.5, 0.7, 0.9}):
   ```
   r ∝ P·exp(t·H(P,Q))
   ```
   Targets high negative log-probability under Q

6. **Exponential family** (β ∈ {0.1, 0.5, 1.0, 2.0, 5.0}):
   ```
   r ∝ P·exp(β·|log(P/Q)|)
   ```
   Exponentially tilts for aggressive targeting

7. **Balanced proposals** (3 variants):
   - Variant 1: `r ∝ √(PQ)·|log(P/Q)|`
   - Variant 2: `r ∝ √(P·|log(P/Q)|·Q/(P+Q))`
   - Variant 3: `r ∝ √(PQ·(log²(P/Q)+1))`

8. **Adaptive log-mixture**:
   ```
   r ∝ P·(1 + |log(P/Q)|)
   ```
   First-order approximation of exponential tilting

9. **Optimal** (theoretical ceiling):
   ```
   r ∝ P|log(P/Q)|
   ```
   Variance-minimizing but intractable for real LLMs

---

## Output

### Console Output

- Distribution statistics (token concentration)
- True KL divergence and theoretical variance
- Per-method estimates with variance and standard error
- Rankings by accuracy, variance, and combined score
- Cross-model comparison tables

### Visualizations

Generated by `create_comprehensive_report()`:
- Distribution overlays (P, Q, and all proposals)
- Proposal effectiveness analysis
- Variance comparison across methods
- ESS (Effective Sample Size) metrics
- Performance scatter plots

---

## Key Metrics

- **Absolute Error**: |estimate - true_KL|
- **Variance**: Empirical variance of estimates
- **Relative Error (%)**: Percentage error relative to true KL
- **Variance Reduction (%)**: Improvement over naive MC
- **ESS (%)**: Effective Sample Size percentage
- **Combined Score**: 0.7×(error/true_KL) + 0.3×(variance/theoretical_variance)

---


## LLM Experiments

**[To be completed]**


### Usage

```bash
```

### Configuration

```python
n_reps =            # Number of repetitions
t_length =            # Token sequence length
temperature =        # Sampling temperature
output_folder = 'tests2/'  # Output directory
```

### Output

Results saved as JSON files containing:
- Generated sequences
- Importance weights
- KL estimates per sequence



